"""
To import this module, use the following code in your script:

import sys
sys.path.append(r'/Users/jirlong/Dropbox/Programming/lib2048')

from importlib import reload
from lib2048 import OpenAIHelper

reload(OpenAIHelper)
aitool = OpenAIHelper.OpenAIHelper()
aitool.ask_gpt("What is the meaning of life?", max_tokens=None)

"""

from openai import OpenAI
import time
from tenacity import retry, stop_after_attempt, wait_random_exponential
import os

class OpenAIHelper:
    def __init__(self):
        self.api_key = os.getenv('OPENAI_API_KEY')
        self.client = OpenAI(api_key = self.api_key)
    
    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
    def ask_gpt(self, prompt, temperature=0, max_tokens=60, original=False):
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=max_tokens,
            n=1,
            stop=None,
            temperature=temperature,
            frequency_penalty=0.0,
            presence_penalty=0.0
        )
        if original:
            return response
        else:
            return response.choices[0].message.content
        
    """
    Default parameters:
        link: https://platform.openai.com/docs/api-reference/completions
        - **`frequency_penalty`**: 如果數值越高，在輸出過程會盡量少用前面輸出的內容，比較不會有重複的字眼。Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
        - **`presence_penalty`**: 如果數值越高，在輸出過程會盡量避免專注在同一主題，會有比較多樣的主題。Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
        - **`temprature`**: 輸出越高，隨機性越高。
        - **`top_p`: `temperature`**的替代方案。如果是0.1的話，代表只取樣機率前10%的Tokens
        - **`max_tokens`**: 生成結果的最大tokens數
        - **`n`**：生成幾個output？
        - **`stop`**: 遇到哪些字就停止輸出（例如換行符號或句號）。
        - **`best_of`**: Generates best_of completions server-side and returns the "best" (the one with the highest log probability per token). Results cannot be streamed.When used with n, best_of controls the number of candidate completions and n specifies how many to return – best_of must be greater than n.
            - Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.
        - **`logit_bias`**: Defaults to `null` Modify the likelihood of specified tokens appearing in the completion.Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.`
    """
        
    
    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
    def detect_stance(self, text, target=None, temperature=0):
        retries = 3
        while retries > 0:
            selection = f"Stance should be one of Supportive, Oppising, Neutral, or Not-A-Stance-toward-the-target"
            prompt = f"Given the following text, determine the stance toward {target}.\n{selection}:\n\n{text}\n\nStance:"
            try:
                response = self.ask_gpt(prompt, temperature)
                content = response.choices[0].message.content.strip()
                return content
            except Exception as e:
                print("Meet: ", e)
                time.sleep(60)
                retries -= 1
        return None
    
    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
    def completion_with_backoff(self, **kwargs):
        return openai.Completion.create(**kwargs)


if __name__ == "__main__":
    aitool = OpenAIHelper()
    response = aitool.ask_gpt("Hello, world!", 0.5, 5)
    print(response)

